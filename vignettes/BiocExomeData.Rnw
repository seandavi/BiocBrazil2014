%\VignetteIndexEntry{Bioconductor Exome Exercises}
%\VignettePackage{BiocBrazil2014}
%\VignetteEngine{knitr::knitr}

\documentclass[12pt]{article}
\usepackage[left=2cm,top=2cm,bottom=3cm,right=2cm,ignoreheadfoot]{geometry}
\pagestyle{empty}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{helvet}
\usepackage[titletoc]{appendix}
\usepackage{tocloft}

\setlength{\parindent}{0em}
\setlength{\parskip}{.5em}

\renewcommand{\familydefault}{\sfdefault}

\newcommand{\Rcode}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\small\texttt{#1}}}
\newcommand{\Rfunction}[1]{\Robject{#1}}
\newcommand{\Rpackage}[1]{\textit{#1}}
\newcommand{\Rclass}[1]{\textit{#1}}

\newcommand{\lowtilde}{\raise.17ex\hbox{$\scriptstyle\mathtt{\sim}$}}

\newcommand{\thetitle}{Differential analysis of RNA-Seq data at the gene level using the DESeq2 package}

\usepackage[pdftitle={\thetitle},%
  pdfauthor={Sean Davis},%
  bookmarks,%
  colorlinks,%
  linktoc=section,%
  linkcolor=RedViolet,%
  citecolor=RedViolet,%
  urlcolor=RedViolet,%
  linkbordercolor={1 1 1},%
  citebordercolor={1 1 1},%
  urlbordercolor={1 1 1},%
  raiselinks,%
  plainpages,%
  pdftex]{hyperref}

\usepackage{cite}
\renewcommand{\floatpagefraction}{0.9}



\usepackage{sectsty}
\sectionfont{\sffamily\bfseries\color{RoyalBlue}\sectionrule{0pt}{0pt}{-1ex}{1pt}}
\subsectionfont{\sffamily\bfseries\color{RoyalBlue}}
\subsubsectionfont{\sffamily\bfseries\color{RoyalBlue}}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\lfoot{}\cfoot{}\rfoot{}
\renewcommand{\headrule}{}
\usepackage[numbers]{natbib}
\usepackage{graphicx}

<<options,echo=FALSE,results='hide'>>=
library(knitr)
opts_chunk$set(fig.show='asis',size='footnotesize',error=FALSE,fig.align='center',fig.pos='!htbp',cache=TRUE)
options(width=70)
@ 
          
%%--Questions and Answers macro; this could be but into a re-usable style file 
\newcounter{questionCounter}
\newcounter{answerCounter}
\newenvironment{question}{%
\refstepcounter{questionCounter}
\par \textbf{Question \arabic{questionCounter}}:}{}
\newenvironment{answer}{%
\refstepcounter{answerCounter}
\par \textbf{Answer \arabic{answerCounter}}:}{}


\title{Using Bioconductor to Characterize Exome \\
  and Genome Sequencing Variants}
\author{Sean Davis}
\date{\today}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}

<<intro,echo=FALSE>>=
library(knitr)
opts_chunk$set(cache=TRUE)
@

New sequencing technologies (here called NGS for simplicity) are revolutionizing all fields of biology.  One area of major interest is the characterization of DNA variation.  In this tutorial, I will be presenting some exercises that showcase some of the tools that Bioconductor has to offer when analyzing exome and genome sequencing studies.  

NGS is still a relatively expensive proposition.  Therefore, several strategies have been developed to target regions of interest in the genome for sequencing.  The most common approach is to target the exome.  Doing so reduces costs by approximately 4-8 fold over whole-genome sequencing while still probing the regions most likely to cause disease.  A recent review \citep{Biesecker2011} highlights some of the pros and cons of using the exome sequencing approach over whole-genome sequencing.  

Several capture technologies exist and new protocols and reagents are being introduced at a staggering pace given the relative simplicity of the idea.  A [[http://www.ncbi.nlm.nih.gov/pubmed/21947028][performance comparison of exome DNA sequencing technologies]] presents a view of the field as of September, 2011.  At the end of the day, though, exome sequencing has had remarkable staying power and will continue to play a role in sequencing experimental design and analysis for some time to come.

\subsection{Goals}
 
A "complete" discussion of all aspects of exome capture sequencing is simply not possible.  The goal of this vignette is to show use cases for R and Bioconductor that answer questions important in exome capture sequencing.  

\begin{itemize}
\item Evaluate a capture technology
\item Evaluate a capture experiment
\item Perform quality control on exome capture variant calls
\item Examine variants resulting from a couple of exome capture datasets
\end{itemize}
  
This vignette is \textit{NOT} a workflow for performing and analysing exome capture data.
   
\subsection{Description of data}

In this tutorial, we will be working with two small datasets, each composed of three exome captured samples.  Data from chromosome 22 have been isolated to keep the datasets manageable.  
   
Both datasets have been aligned to the human reference genome to produce BAM files, processed using GATK tools (see Figure \ref{fig:GATKworkflow}), and then had variants called using samtools.


\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{BP_workflow}
  \caption{Example workflow for postprocessing sequencing data (taken from the GATK website)}
  \label{fig:GATKworkflow}
\end{figure}

The first dataset is a subset of data from the 1000 Genomes pilot.  The data were downloaded from [ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/other\_exome\_alignments/](here) and includes samples NA19909, NA19914, and NA19916.  The data were prepared using the following commands:
   
\begin{verbatim}
samtools view -h ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/other\_exome\_alignments/NA19914/exome\_alignment/NA19914.mapped.illumina.mosaik.ASW.exome.20111114.bam 22 \
  | /usr/local/samtools/samtools view -bS - > NA19914.chr22.bam
samtools view -h ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/other\_exome\_alignments/NA19909/exome\_alignment/NA19909.mapped.illumina.mosaik.ASW.exome.20111114.bam 22 \
  | /usr/local/samtools/samtools view -bS - > NA19909.chr22.bam
samtools view -h ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/other\_exome\_alignments/NA19916/exome\_alignment/NA19916.mapped.illumina.mosaik.ASW.exome.20111114.bam 22 \
  | /usr/local/samtools/samtools view -bS - > NA19916.chr22.bam
samtools index NA19914.chr22.bam
samtools index NA19909.chr22.bam
samtools index NA19916.chr22.bam
\end{verbatim}
   
   The second data set is unpublished data from the [[http://dtp.nci.nih.gov/docs/misc/common\_files/cell\_list.html][NCI-60 cell line panel]].  Chosen to represent common cancers, the NCI-60 panel has been extensively profiled for copy number variation, gene expression, DNA methylation, drug sensitivity, miRNA expression, and limited DNA sequence variation (Sanger sequencing).  The data from three lines, HOP-92, SK-MEL-5, and HCT-15 are included here as illustrative examples. 

CAPTION: Cell line names and cancer types they represent
| Cell Line Name | Cancer Type  |
|----------------|--------------|
| HCT-25         | Colon Cancer |
| HOP-92         | Lung Cancer  |
| SK-MEL-5       | Melanoma     | 

\section{Evaluating the Capture Technologies}

A first step in designing and analysing an exome capture experiment is to evaluate the technology proposed for the capture.  Typically, the manufacturers of a capture kit will provide a urlXXXXhttp://genome.ucsc.edu/FAQ/FAQformat\#format1 (BED file) that describes either the *targets* or the *baits*.  A *target* is a region of the genome which is targeted for sequencing.  In exome capture, the targets are typically the coding exons but may additionally include UTRs, splice sites, or promoter regions.  A *bait* is a sequence region designed to capture around the targets; typically, baits are short oligos that are used to hybridize to the input DNA.  Note that baits and targets are different concepts.  When evaluating a capture strategy, we are typically going to be concerned with how well the baits overlap with our targets.

For each of the two capture experiments, I have supplied the capture regions as BED files.  As a quick representative example, we will look at the Agilent 38MB SureSelect capture kit that was used for the NCI60 capture data.

<<capture10>>=
library(rtracklayer)
ag38mss = import(system.file('extdata/capture/Agilent_SureSelect_Human_All_Exon_38Mb_Kit_hg19.bed',
  package='BasicExomeExample'))
class(ag38mss)
@

As we can see, this is an object of class /UCSCData/.  We are going to be doing some interval operations and examinations on these data, so we first convert to a GRanges object.

<<capture20>>=
ag38mssGR = as(ag38mss,"GRanges")
ag38mssGR
seqlevels(ag38mssGR) = sub('chr','',seqlevels(ag38mssGR))
@

Now, we can begin to look at the data.  What are the sizes of the baits?

<<capture25>>=
summary(width(ag38mssGR))
print(table(width(ag38mssGR)))
@

We might ask the question about how the baits are spaced relative to each other.  In particular, how many baits cover a particular region of the genome?  We can calculate coverage to answer that question.

<<capture27>>=
baitcov = coverage(ag38mssGR)
@

The \textit{baitcov} object is a list of integer \textit{Rle} objects.  We want to know how many bases in the genome have each level of coverage.  The goal, then, is to create a two-column table with coverage level as one column and number of bases (at that coverage level) as the second column.

<<capture28>>=
# make a single long vector of each of the lengths and values of 
# the baitcov object
rl = do.call(c,sapply(baitcov,runLength))
rv = do.call(c,sapply(baitcov,runValue))
# need to use "as.numeric" here to avoid integer overflow
baseCov = aggregate(rl,by=list(rv),function(tmp) {
  return(sum(as.numeric(tmp)))})
colnames(baseCov) = c('coverage','numberOfBases')
baseCov
@

Agilent advertises this as a 38MB capture kit and the result above suggests that this is, indeed, the case.  Interestingly, most bases are captured by only one bait, but some are captured by many more baits than that.  There may be good design reasons for this redundancy.

It is also interesting to examine the extent to which the baits cover known targets.  For the purposes of this exercise, we will be using the UCSC known genes TranscriptDB package to define our target regions of interest.  

<<capture30>>=
library(TxDb.Hsapiens.UCSC.hg19.knownGene)
@

We are usually interested in the coding regions of the genes when performing exome capture experiments, so we pull those regions using the \Rfunction{cds} method.

<<capture50>>=
cdsRegions = cds(TxDb.Hsapiens.UCSC.hg19.knownGene)
# We are being unsafe and ignoring sequence lengths
seqlengths(cdsRegions) <- rep(NA,length(seqlengths(cdsRegions)))
# For the purposes of this tutorial, we will try to stick 
# with the convention of NO 'chr' in chromosome names
seqlevels(cdsRegions) <- sub('chr','',seqlevels(cdsRegions))
@

Because the baits work by hybridization to the fragmented sample library, the baits can probably be extended by about 50 bp to allow for this extended capture sequence.  The \Rfunction{resize} can be used to extend regions by a fixed length.  Also, we can ignore strand since the hybridization process is not strand-specific.

<<capture60>>=
# resize regions to be 50bp larger on each end
ag38mssGRpadded = resize(ag38mssGR,width=width(ag38mssGR)+100,fix="center")
# and fix so strand is '*' since stranded overlaps are not important
strand(ag38mssGRpadded) = '*'
@

Finally, we are ready to count the overlaps between the cdsRegions and the expanded baits (Figure \ref{fig:capture70}).

<<capture70,fig.cap="This figure shows the number of CDS regions that overlap with 0, 1, 2, ... baits.  Note that a significant proportion of CDS regions do not overlap with any baits, at least for this very early capture reagent.">>=
# Get the number of baits overlapping each cdsRegion
CDSOverlapsWithBaits = countOverlaps(cdsRegions,ag38mssGRpadded)
hist(CDSOverlapsWithBaits,xlim=c(0,10),breaks='scott',col='black',
     main='CDS regions versus number of bait overlaps')
@

\begin{question}
Can you find the number of baits with no overlap with any CDS regions?
\end{question}

\begin{answer}
Simply count the number of CDS regions that overlap with the baits and then count the number of baits that have zero overlaps.
<<capture71>>=
BaitOverlapsWithCDS = countOverlaps(ag38mssGRpadded,cdsRegions)
emptyBaits = sum(BaitOverlapsWithCDS==0)
@
So, there appear to be \Sexpr{emptyBaits} baits with no overlap with a CDS region.  Note that these baits may overlap UTR regions or may have been designed to other transcripts not represented in the UCSC KnownGenes annotation.
\end{answer}


\section{Overall capture quality evaluation}

After the exome capture experiment, quality control of the sequencing files, and alignment to the genome are complete, we are often interested to look at the data from a fairly high level.  Exome sequencing is used to reduce cost by focusing sequencing efforts on the exome, so it is common to need to know the relative enrichment over doing whole-genome sequencing.  In other words, what proportion of the reads from the sequencing reaction are on or near the exome?  

Since we are ultimately interested in finding variants in the data and the sensitivity (and specificity) is related to sequencing depth, we sould like to know the coverage at each base in the target regions (the exome).

To examine these issues, we are going to use three samples from the 1000 Genomes project, but we will limit our analyses to only chromosome 22 to keep the analyses snappy.  

\subsection{Capture Enrichment}

If no capture had been used, the reads would be "randomly" spread across the genome.  We are interested in how well our capture reaction worked.  By counting the proportion of aligned reads that fall on our "extended" bait regions versus the number expected if the reads were randomly placed, we can get an estimate of the enrichment availed us by using targeted sequencing over whole-genome sequencing. 

The 1000 Genomes project used a different capture reagent than what we worked with in the first exercise, so we load the 1000 Genomes regions.

<<captureqc10>>=
# This is a non-standard BED format, so must load using 
# read.delim rather than rtracklayer::import
ag1000gss = read.delim(system.file('extdata/capture/20110225.exome.consensus.annotation.bed',
  package='BasicExomeExample'),sep="\t",header=FALSE)
# convert to GRanges
ag1000gssGR = GRanges(seqnames=ag1000gss[,1],
  ranges=IRanges(start=ag1000gss[,2],end=ag1000gss[,3]))
seqlevels(ag1000gssGR) = sub('chr','',seqlevels(ag1000gssGR))
# expand each region by 50bp
ag1000gssGR = resize(ag1000gssGR,width=width(ag1000gssGR)+100)
# collapse overlapping regions onto single regions
ag1000gssGR = reduce(ag1000gssGR)
totallength = sum(width(ag1000gssGR))
@

So, this capture reagent would be expected to capture about \Sexpr{totallength} bp of sequence.

To evaluate capture, we need the actual reads from a sequencing experiment.  This would normally be done using the following code, but since the BAM files are relatively large, I have simply saved the results and load them in the next step.

<<captureqc20>>=
# library(ShortRead)
# alignedGR = readGappedReads('NA19909.chr22.bam')
# load the .Rda file instead of reading the bam, just for efficiency
data('N19909.chr22.alignedreads')
alignedGR
@ 

We limit our capture regions to those on chromosome 22.

<<captureqc30>>=
chr22captureRegions = ag1000gssGR[seqnames(ag1000gssGR)=='22',]
@

Now, we need to calculate the number of reads that overlap a capture region and the number of reads that fall outside the capture regions.

<<captureqc50>>=
incapture = countOverlaps(chr22captureRegions,alignedGR)
outcapture = length(alignedGR)-sum(incapture)
@

And the number of bases of captured and non-captured bases is approximately:

<<captureqc60>>=
capturedBases = sum(width(chr22captureRegions))
# This is an approximation since it ignores the ends of the chromosome
notCapturedBases = sum(width(gaps(chr22captureRegions)))
@

Finally, I calculate the enrichment as a ratio of ratios.  

<<captureqc70>>=
enrichment = (sum(incapture)/capturedBases)/(outcapture/notCapturedBases)
@

So, we got an enrichment of \Sexpr{enrichment} over a random distribution of reads.  There is some sloppiness in the code above, but a metric like this one can certainly pick up a failed experiment and could be used to compare across runs.  Also, while this is an overestimate given that chromosome 22 is acrocentric, this level of enrichment is associated with a good capture sequencing experiment.

\subsection{Base-level coverage}

Base-level coverage, the number of reads that cover each base, is a very important measure of the quality of sequencing experiment.  Since coverage largely determines how well we can call variants, we want to calculate it for our chromosome 22 example.

\begin{question}
  Using the \Rfunction{coverage} method for \Rclass{GRanges} objects, find the per-base coverage for chromosome 22.
\end{question}
\begin{answer}
<<coverage10>>=
cvg = coverage(alignedGR)
# the class of the result
class(cvg)
# and what cvg looks like
cvg
# Finally, get the chromosome 22 coverage
chr22cov = cvg[['22']]
@

The \Robject{chr22cov} variable now represents an \Rclass{Rle} object giving coverage on chromosome 22.
\end{answer}

\begin{question}

\end{question}

\subsubsection{Visualizing Coverage in Genomic Context}

The Gviz package enables UCSC-like track views of data and annotation and is potentially quite useful for visualizing multiple tracks of data on the same plot.  We can use the Gviz package (or the ggbio package; choice is a GOOD THING) to visualize our coverage data in the context of our capture regions to show the relative enrichment for capture regions (Figure \ref{fig:coverage20}).

<<coverage20,fig.cap='Coverage in genomic context in a small region of chromosome 22.'>>=
library(rtracklayer)
if(!require(Gviz)) {
  biocLite('Gviz')
  require(Gviz)
  }
itrack = IdeogramTrack(genome='hg19',chromosome='22')
gtrack = GenomeAxisTrack(genome='hg19')
# funny issue with chromosome names needing to start with 'chr' for Gviz
chr22CR = chr22captureRegions
seqlevels(chr22CR) = paste('chr',seqlevels(chr22CR),sep="")
atrack = AnnotationTrack(chr22CR,chromosome='22',genome='hg19')
# convert our Rle to a regular integer vector
cvgVec = as.integer(cvg$'22')
# and create a data track with it.
dtrack = DataTrack(start=20e6:21e6,
  width=rep(1,1e6+1),
  data=cvgVec[20e6:21e6],chromosome='22',name='Coverage',type='l',genome='hg19')
plotTracks(list(itrack,gtrack,atrack,dtrack),from=20.1e6,to=20.11e6)
@

\subsubsection{GC Content and Coverage}

There is a well-known phenomenon in capture sequencing (indeed, with the Illumina sequencing platform itself) whereby GC content of the region being sequenced affects the coverage in that region.  Using the Biostrings, BSgenome, and BSgenome data packages, it is just a few lines of code to examine how GC content and capture coverage are related.

<<coverage30>>=
library(BSgenome.Hsapiens.UCSC.hg19)
chr22 = Hsapiens$chr22
# Make views on the chromosome 22 sequence that correspond to
# the capture regions
captSeqViews = Views(chr22,ranges(chr22CR))
# And calculate the GC content:
gcContent = letterFrequency(captSeqViews,letters="GC")
totalBases = letterFrequency(captSeqViews,letters="GCAT")
gcPercent = gcContent/totalBases
hist(gcPercent)
@

\Rclass{Views} on a bioconductor data object like the chromosome 22 sequence allow us to perform independent operations (like \Rfunction{letterFrequency}) on specific regions of the object.  In this case, our regions are defined by our capture regions.  We can also use \Rclass{Views} on our coverage data to get the average coverage in each capture region.

<<coverage40>>=
library(ggplot2)
cvgViews = Views(cvg$'22',ranges(chr22CR))
cvgMeans = viewMeans(cvgViews)
gcCoverage = data.frame(Coverage=cvgMeans,
  GCPercent=as.numeric(gcPercent))
ggplot(gcCoverage,
       aes(x=GCPercent,y=Coverage))+geom_point(alpha=0.2)
@

There appears to be a strong relationship between GC content and coverage such that it will be nearly impossible to capture and sequence regions with GC content above about 0.7.

\section{Evaluating Variant Calls}

The primary goal of targeted capture sequencing is to produce a list of variants between a reference genome and the sample being sequenced.  There are a number of software packages for producing variant calls given a bam file (or a set of them) including:

\begin{itemize}
\item GATK Unified Genotyper
\item samtools mpileup
\item VarScan
\item SnvMix
\item ...
\end{itemize}

For the purposes of this little exercise, I have used samtools mpileup like so:

\begin{verbatim}
/usr/local/samtools/samtools mpileup -D -uf \
  ../fasta/human_g1k_v37.fasta *.bam | bcftools view -vcg - > 1kg.chr22.vcf
\end{verbatim}

\subsection{The VCF Format}

The most common format for describing variants with respect to a reference genome is the \href{http://www.1000genomes.org/wiki/Analysis/Variant%20Call%20Format/vcf-variant-call-format-version-41}{VCF format}.  A VCF file is a tab-delimited text format that contains a header describing the context and content of the file, including definitions of Tags that are used in the file to provide details of the variant calls.  After the header, the actual variant calls are stored in sequential lines of text, one per variant.  Here are a few lines of our example 1000 Genomes VCF file.

<<vcf5,echo=TRUE>>=
vcflines = read.delim(system.file('extdata/1000G/1kg.chr22.vcf',
  package='BasicExomeExample'),skip=17,sep="\t",nrow=20)
vcflines[1:15,1:7]
@

I have removed the header, FORMAT, INFO, and specific genotypes to make the output a bit easier to read.  The columns are fairly self-explanatory.  The ID column can also contain the ID of any known variant such as an rsID from the dbSNP database.  The FILTER column can contain user-defined tags for filters that apply to the specific variant in the file, such as "LOW\_QUALITY" or "INDEL".

The INFO and Genotype columns are a bit more complicated, so I will not describe it in detail and refer the reader to the VCF Format specification for specifics.  However, I will point out that each sample is represented by a column in the VCF file.  The GT section of that column will typically be one of "0/0", "0/1", or "1/1" which correspond to homozygous reference, heterozygous, and homozygous for the first variant allele.  If more than one variant allele is present, the presence of that allele is noted by a 2 or 3 in the genotype.

<<vcf7,echo=TRUE>>=
vcflines[1:15,c(8,9,10)]
@

To understand the tags in the INFO and GENOTYPE columns, one must examine the VCF file header:

<<vcf8>>=
readLines(system.file('extdata/1000G/1kg.chr22.vcf',package='BasicExomeExample'),n=10)
@

As an example, the INFO tag "DP" will be a single (Number=1) value of Type=Integer and represents "Raw read depth".  

\subsection{1000 Genomes Data}
The goal in this section is to examine some qualities of the variant calls made on three samples from the 1000 Genomes Exome Pilot data.

We start by using the VariantAnnotation package to load the VCF file into a VCF object.

<<vcf10>>=
library(VariantAnnotation)
vcf = readVcf(system.file('extdata/1kg.chr22.vcf.gz',
  package='BiocBrazil2014'),genome='hg19')
@

\subsubsection{Using Known Sites As a Quality Metric}

One of the first tasks is to look at the overlap of the called variants with known variants.  Here, I am using dbSNP as a source of "known" variants.  There are probably better choices, particularly if one is interested in likely true positive variants.  The current recommendation from the Broad is to use the latest HapMap VCF file, but I leave that as an exercise.  The idea is that "known" variants represent true positives while the "novel" variants (those not in dbSNP) are enriched for false positives.

To get the locations of dbSNP variants, we can conveniently load one of the \Rclass{SNPlocs} packages.  After loading, I convert to a GRanges object.  Note that I am ignoring the actual genotypes and just grabbing the base pair locations of the variants.

<<vcf20>>=
library(SNPlocs.Hsapiens.dbSNP.20120608)
tmpchr22 = getSNPlocs('ch22')
# convert snp locations to a GRanges object
dbsnpchr22 = GRanges(seqnames=rep('22',nrow(tmpchr22)),
  ranges=IRanges(start=tmpchr22$loc,width=1))
@

Finding the overlaps between the variant calls from the VCF file and dbsnp is straightforward.

<<vcf30>>=
ovl = findOverlaps(vcf,dbsnpchr22)
@

\begin{question}
How many of our variants overlap known dbSNP locations?
\end{question}
\begin{answer}
We an either use \Rfunction{countOverlaps} or look at the overlaps that we already calculated.
<<vcf32>>=
sum(countOverlaps(vcf,dbsnpchr22)>0)
# or
length(unique(queryHits(ovl)))
@ 
\end{answer}

It may be instructive to look at the overlap between the capture regions and the variant calls.

<<vcf35>>=
seqlevels(ag1000gssGR)=sub('chr','',seqlevels(ag1000gssGR))
ovl2 = findOverlaps(vcf,ag1000gssGR)
@

The next code block uses \Rclass{VCF} accessors to extract data.  To make things simpler for data exploration, I define a data.frame of metrics from the VCF file.  We will use the resulting data to examine a few quality metrics with regard to known (in dbSNP) and novel (not in dbSNP) variants.

<<vcf40>>=
QD = qual(vcf)/info(vcf)$DP
knownNovel = rep("novel",length(QD))
knownNovel[queryHits(ovl)]="known"
captured = rep(FALSE,length(QD))
captured[queryHits(ovl2)]=TRUE
pv4matrix = do.call(rbind,as.list(info(vcf)$PV4))
df1 = data.frame(QD=QD,knownNovel=knownNovel,
  QUAL=qual(vcf),
  PV1=pv4matrix[,1],PV2=pv4matrix[,2],
  PV3=pv4matrix[,3],PV4=pv4matrix[,4],
  MQ=info(vcf)$MQ,
  VDB=info(vcf)$VDB,DP=info(vcf)$DP)
@

Now, to examine some relationships between the known and novel sites and some VCF file metrics.  Variant quality should be important.

<<vcf45,fig.cap='Density plot stratified by "known" and "novel" variants.  The "known" variants are more likely to be true positive results.  Note that below a quality of about 20, we are more likely to get a "novel" result, meaning that a quality of 20 should be a minimum threshold.'>>=
library(ggplot2)
ggplot(df1,aes(QUAL,fill=knownNovel))+
    geom_density(alpha=0.4)+scale_x_continuous(limits=c(0,60))
@

\begin{question}
  This is a BONUS question.  Use the randomForest package to determine the most important predictors of ``known'' or ``novel''.
\end{question}
\begin{answer}
\href{http://en.wikipedia.org/wiki/Random_forest}{Random forests} is a machine learning algorithm that incorporates a feature selection step.  The features selected in the random forest training procedure are ranked by importance in their ability to classify.  In our case, we want to determine what variables are good at classifying the variants as ``known'' or ``novel''.  
<<vcf47randomforest>>=
library(randomForest)
# randomForest does not deal with NAs, so we simply remove them
df1NoNAs = df1[complete.cases(df1),]
classsize = min(table(df1NoNAs$knownNovel))
rfresult = randomForest(knownNovel ~ .,df1NoNAs,sampsize=c(classsize,classsize))
@ 

Our random forest predictor is not too good:

<<vcf473randomforest>>=
rfresult
@ 

But the list of predictors is still of potential use.  Higher numbers are more important in producing good predictions.  As we might have hoped, QUAL is the best predictor.

<<vcf476randomforest>>=
importance(rfresult)
@ 
\end{answer}



A number of other plots might be of interest.  Feel free to try things out to see if you can find parameters that appear to separate known from novel variants.


\subsubsection{Using Transition/Transversion Ratio To Estimate False Positive Rate}

DNA mutation at a single nucleotide can be classified as either a transition or a transversion.  As can be seen from the Figure \ref{fig:transversion}, transversions are twice as likely as transitions /a priori/.  Hence, the naive ratio of transitions (ti) to transversions (tv) is 0.5.  Figures are from \href{http://www.mun.ca/biology/scarr/Transitions_vs_Transversions.html}{this site}.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{TransitionTransversion}
\caption{DNA substitution mutations are of two types. Transitions are interchanges of two-ring purines (A--G) or of one-ring pyrimidines (C--T): they therefore involve bases of similar shape. Transversions are interchanges of purine for pyrimidine bases, which therefore involve exchange of one-ring and two-ring structures. Note that transversions (tv) are twice as likely as transitions(ti), leading to an expected ratio of ti/tv of 0.5.}
\label{fig:transversion}
\end{figure}

However, a number of biological processes alter this naive ti/tv ratio so that it is above 2 for natural variation across the human genome and 3.3 for the coding regions of the genome.  A partial explanation is given by the next figure which describes the process of "wobble".  A tautomeric shift (change from an amino group to an imino group) leads to a mispairing in replication and results in a transition mutation in the DNA.  This process occurs naturally for each base and results in SNVs being introduced, all of which are transitions (see Figure \ref{fig:spontaneousmutation}).

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{Transition_mutation}
\caption{In the original double-stranded DNA molecule, A in the standard (amino) form pairs with T. During replication, the two strands separate. In the upper diagram, T pairs with A as usual, which replicates the wild-type sequence. In the lower diagram, A has undergone a tautomeric shift to the non-standard (imino) form A', \textit{which pairs with C}. In the next round of replication, the imino A' shifts back to the amino A form, which pairs with T, which again reproduces the wild-type sequence. Replication of the other strand pairs C with G. By comparison with the original molecule, the result is a T--C mutation. A tautomeric shift in one strand has produced a transition mutation in the complementary strand.  This process leads to a ti/tv ratio that is typically much larger than the expected 0.5.}
\label{fig:spontaneousmutation}
\end{figure}


Now, let us assume that the observed ti/tv ratio is a mixture of true positive variants and false positive variants.  

\begin{itemize}
\item If all observed variants are true positives, then we should observe approximately the ti/tv for the exome, which is reliably estimated at 3.3.
\item If all observed variants are false positives, then we should observe the naive expected ti/tv, or 0.5.
\item In reality, the observed ti/tv ratio is a mixture of true and false positives.  
\end{itemize}

Given the observed ti/tv ($titv_{obs}$), the false positive ti/tv ($titv_{fp}$), the true positive ti/tv ($titv_{tp}$), and $\alpha$, the proportion of SNVs that are true positives, we can write:

\begin{equation}
\alpha ( titv_{tp} ) + ( 1 - \alpha ) ( titv_{fp} ) = titv_{obs}
\end{equation}

Solving for 1 - $\alpha$ gives an estimate of the false positive rate.

\begin{equation} \label{eq:fpr}
FPR = 1 - (titv_{obs} - titv_{fp})/(titv_{tp} - titv_{fp})
\end{equation}


Writing a function to calculate $titv_{obs}$ is our first step to using ti/tv ratio as a quality metric.

<<titv>>=
titv = function(object) {
  # takes a VCF object as input
  # returns a list:
  #  table: the table of mutations from reference to alternate allele
  #  ti: The number of transition mutations observed
  #  tv: The number of transversion mutations observed
  #  ratio: The ratio of ti/tv
  refall = as.character(ref(object))
  altall = as.character(unlist(alt(object))[start(PartitioningByEnd(alt(object)))])
  Nrows = (refall=="N" | altall=="N")
  snpsonly = (nchar(refall)==1 & nchar(altall)==1)
  refall = factor(refall[!Nrows & snpsonly],levels=c('A','C','G','T'))
  altall = factor(altall[!Nrows & snpsonly],levels=c('A','C','G','T'))
  y = table(refall,altall)
  ti = y[2,4]+y[4,2]+y[1,3]+y[3,1]
  tv = y[1,2]+y[2,1]+y[1,4]+y[4,1]+y[2,3]+y[3,2]+y[3,4]+y[4,3]
  return(list(table=y,ti=ti,tv=tv,ratio=ti/tv))
  }
@

\begin{question}
Write function to calculate the false positive rate (see equation \ref{eq:fpr}) given an observed ti/tv ratio, the expected ti/tv for false positives, and the expected ti/tv for true positives.
\end{question}
\begin{answer}
I am assuming that the ti/tv ratio is 0.5 for false positives (background expectation) and approximately 3.3 for true positives in the exome.  If we calculate our ti/tv ratio for our dataset, we can plug that in to get an estimate of the false positive rate.
<<vcf70>>=
# takes an observed ti/tv ratio, the expected titv for false positives,
# and the expected titv for true positives; default is appropriate for exome data
fpr = function(titvObs,titvFP=0.5,titvTP=3.3) {
    return(1-(titvObs-titvFP)/(titvTP-titvFP))
}
@
\end{answer}

To ask a couple of relevant questions, we need to limit our variants to the exome.

<<vcf71>>=
library(TxDb.Hsapiens.UCSC.hg19.knownGene) 
cdsRegions = cds(TxDb.Hsapiens.UCSC.hg19.knownGene)
cdsRegions = reduce(cdsRegions)
# We are being unsafe and ignoring sequence lengths
seqlengths(cdsRegions) <- rep(NA,length(seqlengths(cdsRegions)))
# For the purposes of this tutorial, we will try to stick 
# with the convention of NO 'chr' in chromosome names
seqlevels(cdsRegions) <- sub('chr','',seqlevels(cdsRegions))
# overlaps between cdsRegions and our variants
ovl = findOverlaps(cdsRegions,vcf)
vcf2 = vcf[subjectHits(ovl),]
@

The \Robject{vcf2} variable represents our variants that overlap coding regions.  And now, lets look at the estimated false positive rate, based on our ti/tv calculation, of known and novel variants.

<<vcf77>>=
ovl2 = findOverlaps(vcf2,dbsnpchr22)
knownNovel=rep("novel",nrow(vcf2))
knownNovel[queryHits(ovl2)]="known"
titvKnown = titv(vcf2[knownNovel=='known'])
titvNovel = titv(vcf2[knownNovel=='novel'])
titvKnown$ratio
titvNovel$ratio
@

The ti/tv ratio for known sites is close to that empirically expected for exonic variants while the ratio for novel sites is much lower.  An estimate of the FPR is then:

<<vcf78>>=
fpr(titvKnown$ratio)
fpr(titvNovel$ratio)
@

Our 1000 Genomes subset dataset shows very high quality at known variant sites.  However, the "novel" sites appear to be enriched for false positives.


Can we improve on this FPR?  We noticed above that the variant quality, QUAL, distinguished known and novel variants well.  So, it makes sense that QUAL might enrich for true positives.  Let's take a look.

<<vcf79>>=
vcfqual = qual(vcf2)
summary(vcfqual)
qualSeq = seq(5,60,5)
titvByQuality = sapply(qualSeq,function(threshold) {
  return(titv(vcf2[(knownNovel=='novel') & (vcfqual>threshold)])$ratio)})
@

<<vcf795>>=
plot(qualSeq,titvByQuality,main="Ti/Tv Versus Variant Quality",
     sub = "Novel variants only",xlab="Variant Quality Threshold",
     ylab = "Observed Ti/Tv Ratio")
@

At this point, one can begin to play with various metrics to try to enrich both known and novel variant calls with true positives.  This filtration process, which leads to higher specificity, must be balanced against loss in sensitivity.  I leave construction of ROC curves as a further exercise, but note that one can use the known variants as an estimate of true positives.

Consider how you would use other sources of genomic annotation to further improve the quality and biological interpretability of these data. 

\subsection{NCI60 Data}
The NCI60 data are analogous to the 1000 genomes dataset, but the variants in this dataset may be disease-associated.  In fact, the remnants of the original disease etiology are still present in the data.  

<<vcf80>>=
melVcf = readVcf(system.file('extdata/SK-MEL-5.vcf.gz',package='BiocBrazil2014'),genome='hg19')
@

\subsubsection{Exercises}

* See what you can learn about the biology of melanoma by examining the transition/transversion matrix.
* Make a GRanges object from data from the COSMIC database to look for variant overlaps with known cancer variants.

\section{Using SRAdb to Interact With the IGV Browser}

Working with sequence data is often best done interactively in a genome browser, a task
not easily done from R itself. We have found the Integrative Genomics Viewer (IGV) a
high-performance visualization tool for interactive exploration of large, integrated datasets,
increasing usefully for visualizing sequence alignments. In SRAdb, functions startIGV,
load2IGV and load2newIGV provide convenient functionality for R to interact with IGV.
IGV offers a remort control port that allows R to communicate with IGV. The current
command set is fairly limited, but it does allow for some IGV operations to be performed in
the R console. To utilize this functionality, be sure that IGV is set to allow communication via
the "enable port" in the advanced preferences in IGV (it is by default).

<<sradb10,eval=FALSE>>=
if(!require(SRAdb)) {
  library(BiocInstaller)
  biocLite('SRAdb')
  library(SRAdb)
  }
# start IGV on your computer 
startIGV('mm') # 1.2GB memory used
# if the command above does not work, start IGV by hand before continuing

#connect to IGV socket}
sock = IGVsocket()

#Load a BAM from the web into IGV
IGVload(sock,'http://watson.nci.nih.gov/projects/nci60/wes/BAMS/SK-MEL-5_reord_mdups_ralgn_fmate_recal.bam')

# And load the VCF file for the same sample
IGVload(sock,'http://watson.nci.nih.gov/projects/nci60/wes/vcf/SK-MEL-5.vcf')

gotoVCFRecord <- function(vcfObject,sock,idx=1) {
  # takes a VCF object, an IGV socket, and the index 
  # of the variant of interest
  rd = rd = rowData(vcf)
  IGVgoto(sock,sprintf("%s:%d-%d",seqnames(rd)[idx],start(rd)[idx],end(rd)[idx]))
  }
vcf = readVcf(system.file('extdata/SK-MEL-5.vcf.gz',package='BiocBrazil2014'),genome='hg19')
indelSites = which(info(vcf)$INDEL)
# go to the first indel site
gotoVCFRecord(vcf,sock,indelSites[1])
#and take a pretty picture--see your working directory
IGVsnapshot(sock)

@

<<relocateAnswers, echo=FALSE>>=
relocateAnswers = function(con="BiocExomeData.tex"){
  txt = readLines(con=con)
  i1 = grep("\\begin{answer}", txt, fixed=TRUE)
  i2 = grep("\\end{answer}",  txt, fixed=TRUE)
  if(!((length(i1)==length(i2))&&all(i2>i1))) stop("\\begin{answer}/\\end{answer} macros are unbalanced.")
  i = unlist(mapply(`:`, i1, i2))
  res = txt[-i]
  sol = txt[i]
  dest = grep("\\section{Solutions}", res, fixed=TRUE)
  if(length(dest)!=1) stop("\\section{Solutions} not found exactly once.")
  res = c(res[1:dest], sol, res[(dest+1):length(res)])
  writeLines(res, con=con)
}
@ 

\newpage
\section{Solutions}


\section{sessionInfo}

<<sessionInfo>>=
sessionInfo()
@

\bibliographystyle{plain}

\bibliography{BiocExomeData}

\end{document}
